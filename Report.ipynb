{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-agent Reinforcement Learning \n",
    "## Project 3 - Report\n",
    "\n",
    "To sove the multi-agent reinforcement learning problem two DDPG agents where trained. The DDPG algorithm Deep Deterministic Policy Gradient (DDPG) algorithm is part of the A2C / Actor-Critic family. That means the agent consists of two neural networks. The first is the Actor, it takes the state as an input and returns the action to take. The secod network, the critic, takes the state as an input and the action taken by the actor. It returns the Q-Value of the state: critic_net(state, actor_net(state)) -> Q(s,a). \n",
    "\n",
    "Due to the multi-agent environment, there are 2 DDPG Agents that makes in sum 4 networks: 2 actor networks and 2 critic networks. Since DDPG is an offline aglorithm there is a replay buffer implemented to store the experiences by both agents. Practially both agents share this memory.\n",
    "\n",
    "\n",
    "\n",
    "## Architecture:\n",
    "The network of the Actors consists of 2 fully connected (hidden) layers with a size of 400 and 300 units. After each layer is a batch normalization integrated and for all except the output layer the activation funtions are relu-activation-functions. The output layer has a tanh-activation-funtion to give an output between [-1, 1].\n",
    "\n",
    "The critic network is as well build of 2 fully connected (hidden) layers with a size of 256 and 128 units. As with the actor network, after the each layer there is a batch normalization. For activation function the relu function is used\n",
    "\n",
    "\n",
    "\n",
    "## Training\n",
    "After each step in the environment the (state,reward, action, next state)-tuples are saved in the replay buffer with a size of 1e6 tuples. From that memory during the optimization step experience is sampled of with a batchsize of 256 tuples. The optimization happens every step and is repeated 3 times.\n",
    "\n",
    "The critics loss is calculated as the MSE-loss between the expected-Q-value and the target-Q-value as equivalent Q-learning algorithms. The actor loss is calculated by the negative output of the critic network with the inputs of the current state and the predicted action by the actor.\n",
    "\n",
    "Both, actor and critic networks are optimized by the adam optimizer with a learning rate of 1e-3. After each optimization the weights of the networks are updated with a softupdate step with a tau-value of 1e-3. Further a gamma of 0.99 is chosen and a noise is added to the actions to improve exploration (an epsilon value to decrease the influence of the noise to the actions over time).\n",
    "\n",
    "\n",
    "![Report](imgs/Tennis.png)\n",
    "\n",
    "\n",
    "### Further improvements\n",
    "To improve learning the DDPG-Algorithm could be updated to a new developed D4PG-Algorithm that uses several techniques/improvements that were discovered on the value-based methods in reinforcement learning. For D4PG that would imply:\n",
    "\n",
    "- n-step temporal difference calculation\n",
    "- using a prioritized experience replay buffer\n",
    "- replacing the single Q-values from the critic with a probability distribution. Where the Bellman equation is replaced with the Bellman operator that displays the distribution in a simular way as the single Q-value.\n",
    "\n",
    "- as well it would be interesting to see how an on-policy algorithm performs (A2C with and without PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
